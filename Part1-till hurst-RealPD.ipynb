{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================================\n",
    "#making time stamp uniform by Interpolation\n",
    "from scipy import interpolate\n",
    "def preprocess(data):\n",
    "    freq=50\n",
    "    ls=['X','Y','Z']\n",
    "    t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],0.02)\n",
    "    df=pd.DataFrame({'Timestamp':t1})\n",
    "    for i in ls:\n",
    "        fcubic = interpolate.interp1d(data.Timestamp, data[i], kind='cubic')\n",
    "        df[i]=fcubic(t1)\n",
    "    df.columns=['Timestamp','acc_X','acc_Y','acc_Z']\n",
    "    return df\n",
    "\n",
    "#making time stamp uniform by Interpolation \n",
    "def preprocess_real_smartphone(data):\n",
    "    data.rename(columns={'t':'Timestamp','x':'X','y':'Y','z':'Z'},inplace=True)\n",
    "    ls=['X','Y','Z']\n",
    "    freq=round((1/((data.Timestamp.max()/data.Timestamp.shape[0]).round(3))),0)\n",
    "    t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],(data.Timestamp.max()/data.Timestamp.shape[0]).round(3))\n",
    "    #freq=100\n",
    "    #t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],0.01)\n",
    "    df=pd.DataFrame({'Timestamp':t1})\n",
    "    for i in ls:\n",
    "        fcubic = interpolate.interp1d(data.Timestamp, data[i], kind='cubic')\n",
    "        df[i]=fcubic(t1)\n",
    "    df.columns=['Timestamp','acc_X','acc_Y','acc_Z']\n",
    "    return df,freq\n",
    "\n",
    "def preprocess_real_smartwatch(data):\n",
    "    \n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    \n",
    "    data=data[data.device_id==deviceid].reset_index()\n",
    "    data.rename(columns={'t':'Timestamp','x':'X','y':'Y','z':'Z'},inplace=True)\n",
    "   \n",
    "    ls=['X','Y','Z']\n",
    "    #freq=round((1/((data.Timestamp.max()/data.Timestamp.shape[0]).round(3))),0)\n",
    "    freq=50\n",
    "    t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],0.02)\n",
    "    #t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],(data.Timestamp.max()/data.Timestamp.shape[0]).round(3))\n",
    "    df=pd.DataFrame({'Timestamp':t1})\n",
    "        \n",
    "    for i in ls:\n",
    "        fcubic = interpolate.interp1d(data.Timestamp, data[i])\n",
    "        df[i]=fcubic(t1)\n",
    "    df.rename(columns={'X':'acc_X','Y':'acc_Y','Z':'acc_Z'},inplace=True)\n",
    "    return df[['Timestamp','acc_X','acc_Y','acc_Z']],deviceid,freq\n",
    "\n",
    "#==========================================================================================\n",
    "#median filter\n",
    "from scipy.signal import medfilt # import the median filter function\n",
    "def median(signal):# input: numpy array 1D (one column)  \n",
    "    #applying the median filter\n",
    "    return  medfilt(np.array(signal), kernel_size=3) # applying the median filter order3(kernel_size=3)\n",
    "\n",
    "\n",
    "#==========================================================================================\n",
    "#components_selection_one_signal\n",
    "import math # import math library\n",
    "\n",
    "\n",
    "def components_selection_one_signal(t_signal,sampling_freq):\n",
    "    nyq=sampling_freq/float(2) # nyq is the nyquist frequency equal to the half of the sampling frequency[50/2= 25 Hz]\n",
    "\n",
    "    freq1 = 0.3\n",
    "    freq2 = 20\n",
    "\n",
    "    t_signal=np.array(t_signal)\n",
    "    t_signal_length=len(t_signal) # number of points in a t_signal\n",
    "    \n",
    "    # the t_signal in frequency domain after applying fft\n",
    "    f_signal=np.fft.fft(t_signal) # 1D numpy array contains complex values (in C)\n",
    "    \n",
    "    # generate frequencies associated to f_signal complex values\n",
    "    freqs=np.array(np.fft.fftfreq(t_signal_length, d=1/float(sampling_freq))) # frequency values between [-25hz:+25hz]\n",
    "        \n",
    "    df=pd.DataFrame({'freq':abs(freqs),'amplitute':f_signal})\n",
    "    df['f_DC_signal']=np.where(df.freq>freq1,0,df.amplitute)\n",
    "    #df['f_noise_signal']=np.where(df.freq<=freq2,0,df.amplitute)\n",
    "    df['f_body_signal']=np.where(df.freq<=freq1,0,np.where(df.freq>freq2,0,df.amplitute))\n",
    "\n",
    "    \n",
    "    # Inverse the transformation of signals in freq domain #\n",
    "    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n",
    "    t_DC_component= np.fft.ifft(np.array(df['f_DC_signal'])).real\n",
    "    t_body_component= np.fft.ifft(np.array(df['f_body_signal'])).real\n",
    "    #t_noise=np.fft.ifft(np.array(df['f_noise_signal'])).real\n",
    "    t_noise=[]\n",
    "    \n",
    "    #total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n",
    "                                     #  by substracting noise from t_signal (the original signal).\n",
    "    total_component=[]\n",
    "    \n",
    "    # return outputs mentioned earlier\n",
    "    return (total_component,t_DC_component,t_body_component,t_noise) \n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "#Define verify gravity function\n",
    "def mag_3_signals(df): # Euclidian magnitude\n",
    "    return np.array(np.sqrt(np.square(df).sum(axis=1)))\n",
    "\n",
    "def verify_gravity(data):\n",
    "    \n",
    "    acc_x=np.array(data['acc_X']) # copy acc_X column from dataframe in raw_dic having the key mentioned above\n",
    "    acc_y=np.array(data['acc_Y'])# copy acc_Y column  from dataframe in raw_dic having the key mentioned above\n",
    "    acc_z=np.array(data['acc_Z'])# copy acc_Z column  from dataframe in raw_dic having the key mentioned above\n",
    "\n",
    "    # apply the filtering method to acc_[X,Y,Z] and store gravity components\n",
    "    grav_acc_X=components_selection_one_signal(acc_x)[1] \n",
    "    grav_acc_Y=components_selection_one_signal(acc_y)[1]\n",
    "    grav_acc_Z=components_selection_one_signal(acc_z)[1]\n",
    "    \n",
    "    # calculating gravity magnitude signal\n",
    "    grav_acc_mag=mag_3_signals(grav_acc_X, grav_acc_Y,grav_acc_Z)\n",
    "    print('mean value = ',round((sum(grav_acc_mag) / len(grav_acc_mag)),3),' g')\n",
    "    \n",
    "#=================================================================================================================    \n",
    "#Define jerking and magnitude functions\n",
    "def jerk_one_signal(signal,sampling_freq):\n",
    "    signal=pd.DataFrame(signal)\n",
    "    jerk=(signal.shift(-1)-signal)*sampling_freq\n",
    "    return np.array(jerk.dropna()).transpose()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDs_Labels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "cis_pd_testing_id=pd.read_csv('test_data_Id/cis-pd.CIS-PD_Test_Data_IDs.csv')\n",
    "real_pd_testing_id=pd.read_csv('test_data_Id/real-pd.REAL-PD_Test_Data_IDs.csv')\n",
    "\n",
    "#Training Data\n",
    "cis_pd_training_id=pd.read_csv('data_labels/CIS-PD_Training_Data_IDs_Labels.csv')\n",
    "real_pd_training_id=pd.read_csv('data_labels/REAL-PD_Training_Data_IDs_Labels.csv')\n",
    "\n",
    "#Ancillary Data\n",
    "cis_pd_ancillary_id=pd.read_csv('data_labels/CIS-PD_Ancillary_Data_IDs_Labels.csv')\n",
    "real_pd_ancillary_id=pd.read_csv('data_labels/REAL-PD_Ancillary_Data_IDs_Labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_domain_signal(data,sampling_freq):\n",
    "    time_sig_df=pd.DataFrame()\n",
    "    for column in ['acc_X','acc_Y','acc_Z']:\n",
    "        t_signal=np.array(data[column])\n",
    "        #med_filtred=median(t_signal)\n",
    "        med_filtred=(t_signal)\n",
    "        _,grav_acc,body_acc,_=components_selection_one_signal(med_filtred,sampling_freq)\n",
    "        body_acc_jerk=jerk_one_signal(body_acc,sampling_freq)\n",
    "        time_sig_df['t_body_'+column]=body_acc[:-1]\n",
    "        time_sig_df['t_grav_'+column]= grav_acc[:-1]\n",
    "        time_sig_df['t_body_acc_jerk_'+column[-1]]=body_acc_jerk\n",
    "\n",
    "    # all 15 axial signals generated above are reordered to facilitate magnitudes signals generation\n",
    "    new_columns_ordered=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n",
    "                              't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n",
    "                              't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z']\n",
    "\n",
    "\n",
    "    # create new dataframe to order columns\n",
    "    time_sig_df=time_sig_df[new_columns_ordered]\n",
    "\n",
    "    # Magnitude Features\n",
    "    for i in range(0,9,3):\n",
    "        mag_col_name=new_columns_ordered[i][:-1]+'mag'# Create the magnitude column name related to each 3-axial signals\n",
    "        time_sig_df[mag_col_name]=mag_3_signals(time_sig_df[new_columns_ordered[i:i+3]]) # store the signal_mag with its appropriate column name\n",
    "\n",
    "    return(time_sig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import fftpack # import fftpack to use all fft functions\n",
    "from numpy.fft import *\n",
    "\n",
    "# fast_fourier_transform_one_signal \n",
    "def fast_fourier_transform_one_signal(t_signal):\n",
    "    return np.abs(np.fft.rfft(t_signal))\n",
    "\n",
    "# fast fourier transform for data frames\n",
    "def fast_fourier_transform(t_window,sampling_freq):\n",
    "    f_window=pd.DataFrame() \n",
    "    for column in t_window.columns: \n",
    "        if 'grav' not in column: # verify if time domain signal is not related to gravity components\n",
    "            t_signal=np.array(t_window[column]) # convert the column to a 1D numpy array\n",
    "            f_signal= fast_fourier_transform_one_signal(t_signal) # apply the function defined above to the column\n",
    "            f_window[\"f_\"+column[2:]]=f_signal # storing the frequency signal in f_window with an appropriate column name\n",
    "    dfreq=np.array(np.fft.rfftfreq(len(t_signal), d=1/float(sampling_freq))) \n",
    "    return f_window,dfreq # return the frequency domain window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Axial Features Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "def mean_axial(df):\n",
    "    array=np.array(df) # convert dataframe into 2D numpy array for efficiency\n",
    "    mean_vector = list(array.mean(axis=0)) # calculate the mean value of each column\n",
    "    return mean_vector # return mean vetor\n",
    "# std\n",
    "def std_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    std_vector = list(array.std(axis=0))# calculate the standard deviation value of each column\n",
    "    return std_vector\n",
    "\n",
    "# mad\n",
    "from statsmodels.robust import mad as median_deviation # import the median deviation function\n",
    "def mad_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    mad_vector = list(median_deviation(array,axis=0)) # calculate the median deviation value of each column\n",
    "    return mad_vector\n",
    "\n",
    "# max\n",
    "\n",
    "def max_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    max_vector=list(array.max(axis=0))# calculate the max value of each column\n",
    "    return max_vector\n",
    "# min\n",
    "def min_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    min_vector=list(array.min(axis=0))# calculate the min value of each column\n",
    "    return min_vector\n",
    "# IQR\n",
    "from scipy.stats import iqr as IQR # import interquartile range function (Q3(column)-Q1(column))\n",
    "def IQR_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    IQR_vector=list(np.apply_along_axis(IQR,0,array))# calculate the inter quartile range value of each column\n",
    "    return IQR_vector\n",
    "\n",
    "\n",
    "# Entropy\n",
    "from scipy.stats import entropy # import the entropy function\n",
    "def entropy_axial(df):\n",
    "    array=np.array(df)# convert dataframe into 2D numpy array for efficiency\n",
    "    entropy_vector=list(np.apply_along_axis(entropy,0,abs(array)))# calculate the entropy value of each column\n",
    "    return entropy_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Magnitude Features Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "def mean_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    mean_value = float(array.mean())\n",
    "    return mean_value\n",
    "\n",
    "# std: standard deviation of mag column\n",
    "def std_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    std_value = float(array.std()) # std value \n",
    "    return std_value\n",
    "\n",
    "# mad: median deviation\n",
    "def mad_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    mad_value = float(median_deviation(array))# median deviation value of mag_column\n",
    "    return mad_value\n",
    "\n",
    "# max\n",
    "def max_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    max_value=float(array.max()) # max value \n",
    "    return max_value\n",
    "# min\n",
    "def min_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    min_value= float(array.min()) # min value\n",
    "    return min_value\n",
    "\n",
    "# IQR\n",
    "def IQR_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    IQR_value=float(IQR(array))# Q3(column)-Q1(column)\n",
    "    return IQR_value\n",
    "\n",
    "# Entropy\n",
    "def entropy_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    entropy_value=float(entropy(array)) # entropy signal\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Axial Features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sma\n",
    "def t_sma_axial(df):\n",
    "    array=np.array(df)\n",
    "    sma_axial=float(abs(array).sum())/float(3) # sum of areas under each signal\n",
    "    return sma_axial # return sma value\n",
    "\n",
    "# energy\n",
    "def t_energy_axial(df):\n",
    "    array=np.array(df)\n",
    "    energy_vector=list((array**2).sum(axis=0)) # energy value of each df column\n",
    "    return energy_vector # return energy vector energy_X,energy_Y,energy_Z\n",
    "\n",
    "# define the arbugr function\n",
    "#auto regression coefficients with using burg method with order from 1 to 4\n",
    "from spectrum import *\n",
    "\n",
    "##############################################################################################\n",
    "# I took this function as it is from this link ------>    https://github.com/faroit/freezefx/blob/master/fastburg.py\n",
    "# This fucntion and the original function arburg in the library spectrum generate the same first 3 coefficients \n",
    "#for all windows the original burg method is low and for some windows it cannot generate all 4th coefficients \n",
    "\n",
    "def _arburg2(X, order):\n",
    "    \"\"\"This version is 10 times faster than arburg, but the output rho is not correct.\n",
    "    returns [1 a0,a1, an-1]\n",
    "    \"\"\"\n",
    "    x = numpy.array(X)\n",
    "    N = len(x)\n",
    "\n",
    "    if order == 0.:\n",
    "        raise ValueError(\"order must be > 0\")\n",
    "\n",
    "    # Initialisation\n",
    "    # ------ rho, den\n",
    "    rho = sum(abs(x)**2.) / N  # Eq 8.21 [Marple]_\n",
    "    den = rho * 2. * N\n",
    "\n",
    "    # ------ backward and forward errors\n",
    "    ef = numpy.zeros(N, dtype=complex)\n",
    "    eb = numpy.zeros(N, dtype=complex)\n",
    "    for j in range(0, N):  # eq 8.11\n",
    "        ef[j] = x[j]\n",
    "        eb[j] = x[j]\n",
    "\n",
    "    # AR order to be stored\n",
    "    a = numpy.zeros(1, dtype=complex)\n",
    "    a[0] = 1\n",
    "    # ---- rflection coeff to be stored\n",
    "    ref = numpy.zeros(order, dtype=complex)\n",
    "\n",
    "    E = numpy.zeros(order+1)\n",
    "    E[0] = rho\n",
    "\n",
    "    for m in range(0, order):\n",
    "        # print m\n",
    "        # Calculate the next order reflection (parcor) coefficient\n",
    "        efp = ef[1:]\n",
    "        ebp = eb[0:-1]\n",
    "        # print efp, ebp\n",
    "        num = -2. * numpy.dot(ebp.conj().transpose(), efp)\n",
    "        den = numpy.dot(efp.conj().transpose(),  efp)\n",
    "        den += numpy.dot(ebp,  ebp.conj().transpose())\n",
    "        ref[m] = num / den\n",
    "\n",
    "        # Update the forward and backward prediction errors\n",
    "        ef = efp + ref[m] * ebp\n",
    "        eb = ebp + ref[m].conj().transpose() * efp\n",
    "\n",
    "        # Update the AR coeff.\n",
    "        a.resize(len(a)+1)\n",
    "        a = a + ref[m] * numpy.flipud(a).conjugate()\n",
    "\n",
    "        # Update the prediction error\n",
    "        E[m+1] = numpy.real((1 - ref[m].conj().transpose() * ref[m])) * E[m]\n",
    "        # print 'REF', ref, num, den\n",
    "    return a, E[-1], ref\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "# to generate arburg (order 4) coefficents for 3 columns [X,Y,Z]\n",
    "def t_arburg_axial(df):\n",
    "    # converting signals to 1D numpy arrays for efficiency\n",
    "    array_X=np.array(df[df.columns[0]])\n",
    "    array_Y=np.array(df[df.columns[1]])\n",
    "    array_Z=np.array(df[df.columns[2]])\n",
    "    \n",
    "    AR_X = list(_arburg2(array_X,4)[0][1:].real) # list contains real parts of all 4th coefficients generated from signal_X\n",
    "    AR_Y = list(_arburg2(array_Y,4)[0][1:].real) # list contains real parts of all 4th coefficients generated from signal_Y\n",
    "    AR_Z = list(_arburg2(array_Z,4)[0][1:].real) # list contains real parts of all 4th coefficients generated from signal_Z\n",
    "    \n",
    "    # selecting [AR1 AR2 AR3 AR4] real components for each axis concatenate them in one vector\n",
    "    AR_vector= AR_X + AR_Y+ AR_Z\n",
    "    \n",
    "    \n",
    "    # AR_vector contains 12 values 4values per each axis \n",
    "    return AR_vector\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "def t_corr_axial(df): # it returns 3 correlation features per each 3-axial signals in  time_window\n",
    "    \n",
    "    array=np.array(df)\n",
    "    \n",
    "    Corr_X_Y=float(pearsonr(array[:,0],array[:,1])[0]) # correlation value between signal_X and signal_Y\n",
    "    Corr_X_Z=float(pearsonr(array[:,0],array[:,2])[0]) # correlation value between signal_X and signal_Z\n",
    "    Corr_Y_Z=float(pearsonr(array[:,1],array[:,2])[0]) # correlation value between signal_Y and signal_Z\n",
    "    \n",
    "    corr_vector =[Corr_X_Y, Corr_X_Z, Corr_Y_Z] # put correlation values in list\n",
    "    \n",
    "    return corr_vector \n",
    " \n",
    "#hurst exponent\n",
    "def hurst(signal):\n",
    "    \"\"\"\n",
    "    **Experimental**/untested implementation taken from:\n",
    "    http://drtomstarke.com/index.php/calculation-of-the-hurst-exponent-to-test-for-trend-and-mean-reversion/\n",
    "    Use at your own risks.\n",
    "    \"\"\"\n",
    "    \n",
    "    signal=np.array(signal)\n",
    "    tau = []; lagvec = []\n",
    "    #  Step through the different lags\n",
    "    for lag in range(2,20):\n",
    "    #  produce price difference with lag\n",
    "        pp = np.subtract(signal[lag:],signal[:-lag])\n",
    "    #  Write the different lags into a vector\n",
    "        lagvec.append(lag)\n",
    "    #  Calculate the variance of the difference vector\n",
    "        tau.append(np.std(pp))\n",
    "    #  linear fit to double-log graph (gives power)\n",
    "    m = np.polyfit(np.log10(lagvec),np.log10(tau),1)\n",
    "    # calculate hurst\n",
    "    hurst = m[0]\n",
    "    return hurst\n",
    "\n",
    "\n",
    "# to generate hurst  for 3 columns [X,Y,Z]\n",
    "def t_hurst_axial(df):\n",
    "    return list(df.apply(hurst,axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Axial Features PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_axial_features_generation(t_window):\n",
    "    \n",
    "    # select axial columns : the first 9 columns\n",
    "    axial_columns=t_window.columns[0:9]\n",
    "    \n",
    "    # select axial columns in a dataframe\n",
    "    axial_df=t_window[axial_columns]\n",
    "    \n",
    "    ## a list will contain all axial features values resulted from applying: \n",
    "    #  common axial features functions and time axial features functions to all time domain signals in t_window\n",
    "    t_axial_features=[]\n",
    "    for col in range(0,9,3):\n",
    "        df=axial_df[axial_columns[col:col+3]] # select each group of 3-axial signal: signal_name[X,Y,Z]\n",
    "        \n",
    "        # apply all common axial features functions and time axial features functions to each 3-axial signals dataframe\n",
    "        mean_vector   = mean_axial(df) # 3values\n",
    "        std_vector    = std_axial(df) # 3 values\n",
    "        mad_vector    = mad_axial(df)# 3 values\n",
    "        max_vector    = max_axial(df)# 3 values\n",
    "        min_vector    = min_axial(df)# 3 values\n",
    "        sma_value     = t_sma_axial(df)# 1 value\n",
    "        energy_vector = t_energy_axial(df)# 3 values\n",
    "        IQR_vector    = IQR_axial(df)# 3 values\n",
    "        entropy_vector= entropy_axial(df)# 3 values\n",
    "        AR_vector     = t_arburg_axial(df)# 3 values\n",
    "        corr_vector   = t_corr_axial(df)# 3 values\n",
    "        hurst_vector  = t_hurst_axial(df)# 3 values\n",
    "        # 40 value per each 3-axial signals\n",
    "        t_3axial_vector= mean_vector + std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + AR_vector + corr_vector+hurst_vector\n",
    "        \n",
    "        # append these features to the global list of features\n",
    "        t_axial_features= t_axial_features+ t_3axial_vector\n",
    "    \n",
    "    # t_axial_features contains 200 values = 40 value per each 3axial x 5 tri-axial-signals[X,Y,Z]\n",
    "    return t_axial_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Time Magnitudes Features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate time magnitude features\n",
    "\n",
    "# sma: signal magnitude area\n",
    "def t_sma_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    sma_mag=float(abs(array).sum())# signal magnitude area of one mag column\n",
    "    return sma_mag\n",
    "\n",
    "# energy\n",
    "def t_energy_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    energy_value=float((array**2).sum()) # energy of the mag signal\n",
    "    return energy_value\n",
    "\n",
    "\n",
    "\n",
    "# arburg: auto regression coefficients using the burg method\n",
    "def t_arburg_mag(mag_column):\n",
    "    \n",
    "    array = np.array(mag_column)\n",
    "    \n",
    "    AR_vector= list(_arburg2(array,4)[0][1:].real) # AR1, AR2, AR3, AR4 of the mag column\n",
    "    #print(AR_vector)\n",
    "    return AR_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Magnitude Features PipLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_mag_features_generation(t_window):\n",
    "    \n",
    "    # select mag columns : the last 3 columns in a time domain window\n",
    "    \n",
    "    mag_columns=t_window.columns[9:] # mag columns' names\n",
    "    mag_columns=t_window[mag_columns] # mag data frame\n",
    "    \n",
    "    t_mag_features=[] # a global list will contain all time domain magnitude features\n",
    "    \n",
    "    for col in mag_columns: # iterate throw each mag column\n",
    "        \n",
    "        mean_value   = mean_mag(mag_columns[col]) # 1 value\n",
    "        std_value    = std_mag(mag_columns[col])# 1 value\n",
    "        mad_value    = mad_mag(mag_columns[col])# 1 value\n",
    "        max_value    = max_mag(mag_columns[col])# 1 value\n",
    "        min_value    = min_mag(mag_columns[col])# 1 value\n",
    "        sma_value    = t_sma_mag(mag_columns[col])# 1 value\n",
    "        energy_value = t_energy_mag(mag_columns[col])# 1 value\n",
    "        IQR_value    = IQR_mag(mag_columns[col])# 1 value\n",
    "        entropy_value= entropy_mag(mag_columns[col])# 1 value\n",
    "        #hurst_vector    = hurst(mag_columns[col])# 1 value\n",
    "        AR_vector    = t_arburg_mag(mag_columns[col])# 1 value\n",
    "        \n",
    "        # 13 value per each t_mag_column\n",
    "        col_mag_values = [mean_value, std_value, mad_value, max_value, min_value, sma_value, \n",
    "                          energy_value,IQR_value, entropy_value]+ AR_vector\n",
    "        \n",
    "        # col_mag_values will be added to the global list\n",
    "        t_mag_features= t_mag_features+ col_mag_values\n",
    "    \n",
    "    # t_mag_features contains 65 values = 13 values (per each t_mag_column) x 5 (t_mag_columns)\n",
    "    return t_mag_features\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Features names Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features_names():\n",
    "    # Generating time feature names\n",
    "    \n",
    "    # time domain axial signals' names\n",
    "    t_axis_signals=[['t_body_acc_X','t_body_acc_Y','t_body_acc_Z'],\n",
    "                    ['t_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z'],\n",
    "                    ['t_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z']]\n",
    "    \n",
    "    # time domain magnitude signals' names\n",
    "    magnitude_signals=['t_body_acc_Mag','t_grav_acc_Mag','t_body_acc_jerk_Mag']\n",
    "\n",
    "    # functions' names:\n",
    "    t_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n",
    "\n",
    "    t_one_input_features_name2=['_energy()','_iqr()','_entropy()']\n",
    "\n",
    "    t_one_input_features_name3=['_AR1()','_AR2()','_AR3()','_AR4()']\n",
    "\n",
    "    correlation_columns=['_Corr(X,Y)','_Corr(X,Z)','_Corr(Y,Z)']\n",
    "\n",
    "    t_one_input_features_name4=['_hurst()']\n",
    "    \n",
    "\n",
    "    features=[]# Empty list : it will contain all time domain features' names\n",
    "    \n",
    "    for columns in t_axis_signals: # iterate throw  each group of 3-axial signals'\n",
    "        \n",
    "        for feature in t_one_input_features_name1: # iterate throw the first list of functions names\n",
    "            \n",
    "            for column in columns: # iterate throw each axial signal in that group\n",
    "                \n",
    "                newcolumn=column[:-2]+feature+column[-2:] # build the feature name\n",
    "                features.append(newcolumn) # add it to the global list\n",
    "        \n",
    "        sma_column=column[:-2]+'_sma()' # build the feature name sma related to that group\n",
    "        features.append(sma_column) # add the feature to the list\n",
    "        \n",
    "        for feature in t_one_input_features_name2: # same process for the second list of features functions\n",
    "            for column in columns:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                features.append(newcolumn)\n",
    "        \n",
    "        for column in columns:# same process for the third list of features functions\n",
    "            for feature in t_one_input_features_name3:\n",
    "                newcolumn=column[0:-2]+feature+column[-2:]\n",
    "                features.append(newcolumn)\n",
    "        \n",
    "        for feature in correlation_columns: # adding correlations features\n",
    "            newcolumn=column[0:-2]+feature\n",
    "            features.append(newcolumn)\n",
    "            \n",
    "        for feature in t_one_input_features_name4: # adding correlations features\n",
    "            for column in columns:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                features.append(newcolumn)\n",
    "        \n",
    "\n",
    "    for columns in magnitude_signals: # iterate throw time domain magnitude column names\n",
    "\n",
    "        # build feature names related to that column\n",
    "        #list 1\n",
    "        for feature in t_one_input_features_name1:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "        # sma feature name\n",
    "        sma_column=columns+'_sma()'\n",
    "        features.append(sma_column)\n",
    "        \n",
    "        # list 2\n",
    "        for feature in t_one_input_features_name2: \n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "            \n",
    "       \n",
    "        # list 3\n",
    "        for feature in t_one_input_features_name3:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "            \n",
    "        \n",
    "    ###########################################################################################################\n",
    "    time_list_features=features\n",
    "    \n",
    "    return time_list_features # return all time domain features' names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Axial features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sma\n",
    "def f_sma_axial(df):\n",
    "    array=np.array(df)\n",
    "    sma_value=float((abs(array)/math.sqrt(array.shape[0])).sum())/float(3) # sma value of 3-axial f_signals\n",
    "    return sma_value\n",
    "\n",
    "\n",
    "# energy\n",
    "def f_energy_axial(df):\n",
    "    array=np.array(df)\n",
    "    # spectral energy vector\n",
    "    energy_vector=list((array**2).sum(axis=0)/float(len(array))) # energy of: f_signalX,f_signalY, f_signalZ\n",
    "    return energy_vector # enrgy veactor=[energy(signal_X),energy(signal_Y),energy(signal_Z)]\n",
    "\n",
    "#Max Inds and Mean_Freq Functions\n",
    "# max_Inds\n",
    "def f_max_Inds_axial(df,dfreq):\n",
    "    array=np.array(df)\n",
    "    max_Inds_X =dfreq[array[:,0].argmax()] # return the frequency related to max value of f_signal X\n",
    "    max_Inds_Y =dfreq[array[:,1].argmax()] # return the frequency related to max value of f_signal Y\n",
    "    max_Inds_Z =dfreq[array[:,2].argmax()] # return the frequency related to max value of f_signal Z\n",
    "    max_Inds_vector= [max_Inds_X,max_Inds_Y,max_Inds_Z]# put those frequencies in a list\n",
    "    return max_Inds_vector\n",
    "\n",
    "# mean freq()\n",
    "def f_mean_Freq_axial(df,dfreq):\n",
    "    array=np.array(df)\n",
    "    # sum of( freq_i * f_signal[i])/ sum of signal[i]\n",
    "    mean_freq_X = np.dot(dfreq,array[:,0]).sum() / float(array[:,0].sum()) #  frequencies weighted sum using f_signalX\n",
    "    mean_freq_Y = np.dot(dfreq,array[:,1]).sum() / float(array[:,1].sum()) #  frequencies weighted sum using f_signalY \n",
    "    mean_freq_Z = np.dot(dfreq,array[:,2]).sum() / float(array[:,2].sum()) #  frequencies weighted sum using f_signalZ\n",
    "    mean_freq_vector=[mean_freq_X,mean_freq_Y,mean_freq_Z] # vector contain mean frequencies[X,Y,Z]\n",
    "    return  mean_freq_vector\n",
    "\n",
    "\n",
    "# Skewness & Kurtosis Functions\n",
    "from scipy.stats import kurtosis       # kurtosis function\n",
    "from scipy.stats import skew           # skewness function\n",
    "    \n",
    "def f_skewness_and_kurtosis_axial(df):\n",
    "    array=np.array(df)\n",
    "    skew_axial=list(skew(array,axis=0))\n",
    "    kur_axial=list(kurtosis(array,axis=0))\n",
    "    skew_kur_3axial_vector = [i for tup in zip(skew_axial,kur_axial) for i in tup]\n",
    "    return  skew_kur_3axial_vector\n",
    "\n",
    "\n",
    "\n",
    "#f_one_band_energy\n",
    "def f_one_band_energy(psd, bands,dfreq):\n",
    "    psd = np.abs(np.array(psd))**2\n",
    "    bands = np.asarray(bands)\n",
    "    freq_limits_low = np.concatenate([[1],bands])\n",
    "    freq_limits_up = np.concatenate([bands, [25]])\n",
    "    power_per_band_mean = [np.mean(psd[np.bitwise_and(dfreq >= low, dfreq<up)])\n",
    "            for low,up in zip(freq_limits_low, freq_limits_up)]\n",
    "    return power_per_band_mean\n",
    "\n",
    "#spectral_entropy\n",
    "def spectral_entropy(psd, bands,dfreq):\n",
    "    psd = np.abs(np.array(psd))**2\n",
    "    psd2 =psd/np.sum(psd) # psd as a pdf (normalised to one)\n",
    "    bands = np.asarray(bands)\n",
    "    freq_limits_low = np.concatenate([[1],bands])\n",
    "    freq_limits_up = np.concatenate([bands, [25]])\n",
    "    power_per_band = [np.sum(psd2[np.bitwise_and(dfreq >= low, dfreq<up)])\n",
    "        for low,up in zip(freq_limits_low, freq_limits_up)]\n",
    "    power_per_band=np.asarray(power_per_band)\n",
    "    power_per_band= power_per_band[ power_per_band > 0]\n",
    "    t=[- np.sum(power_per_band * np.log2(power_per_band))]\n",
    "    return t\n",
    "\n",
    "#Bands Energy FUNCTIONS\n",
    "B1=[4,7,10,13,16,19,22] \n",
    "B2=[7,13,19]\n",
    "B3=[12]\n",
    "\n",
    "\n",
    "def f_all_bands_energy_axial(df,dfreq): # df is dataframe contain 3 columns (3-axial f_signals [X,Y,Z])\n",
    "    E_3_axis =[]\n",
    "    SE_3_axis =[]\n",
    "    array=np.array(df)\n",
    "    for i in range(0,3): # iterate throw signals\n",
    "        E1=f_one_band_energy(array[:,i],B1,dfreq) # energy bands1 values of f_signal\n",
    "        E2=f_one_band_energy(array[:,i],B2,dfreq)# energy bands2 values of f_signal\n",
    "        E3=f_one_band_energy(array[:,i],B3,dfreq)# energy bands3 values of f_signal\n",
    "        E_one_axis = E1+E2+E3 # list of energy bands values of one f_signal\n",
    "        E_3_axis= E_3_axis + E_one_axis # add values to the global list\n",
    "        \n",
    "        SE1=spectral_entropy(array[:,i],B1,dfreq) # Senergy bands1 values of f_signal\n",
    "        SE2=spectral_entropy(array[:,i],B2,dfreq)# Senergy bands2 values of f_signal\n",
    "        SE3=spectral_entropy(array[:,i],B3,dfreq)# Senergy bands3 values of f_signal\n",
    "        SE_one_axis = SE1+SE2+SE3 # list of energy bands values of one f_signal\n",
    "        SE_3_axis= SE_3_axis + SE_one_axis # add values to the global list        \n",
    "    return E_3_axis+SE_3_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency axial features PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_axial_features_generation(f_window,dfreq):\n",
    "    \n",
    "    \n",
    "    axial_columns=f_window.columns[0:6] # select frequency axial column names\n",
    "    axial_df=f_window[axial_columns] # select frequency axial signals in one dataframe\n",
    "    f_all_axial_features=[] # a global list will contain all frequency axial features values\n",
    "    \n",
    "    \n",
    "    \n",
    "    for col in range(0,6,3):# iterate throw each group of frequency axial signals in a window\n",
    "        \n",
    "        df=axial_df[axial_columns[col:col+3]]  # select each group of 3-axial signals\n",
    "      \n",
    "        # mean\n",
    "        mean_vector                  = mean_axial(df) # 3 values\n",
    "        # std\n",
    "        std_vector                   = std_axial(df) # 3 values\n",
    "        # mad\n",
    "        mad_vector                   = mad_axial(df) # 3 values\n",
    "        # max\n",
    "        max_vector                   = max_axial(df) # 3 values\n",
    "        # min\n",
    "        min_vector                   = min_axial(df) # 3 values\n",
    "        # sma\n",
    "        sma_value                    = f_sma_axial(df)\n",
    "        # energy\n",
    "        energy_vector                = f_energy_axial(df)# 3 values\n",
    "        # IQR\n",
    "        IQR_vector                   = IQR_axial(df) # 3 values\n",
    "        # entropy\n",
    "        entropy_vector               = entropy_axial(df) # 3 values\n",
    "        # max_inds\n",
    "        max_inds_vector              = f_max_Inds_axial(df,dfreq)# 3 values\n",
    "        # mean_Freq\n",
    "        mean_Freq_vector             = f_mean_Freq_axial(df,dfreq)# 3 values\n",
    "        # skewness and kurtosis\n",
    "        skewness_and_kurtosis_vector = f_skewness_and_kurtosis_axial(df)# 6 values\n",
    "        # bands energy\n",
    "        bands_energy_vector          = f_all_bands_energy_axial(df,dfreq) # 42 values\n",
    "\n",
    "        # append all values of each 3-axial signals in a list\n",
    "        f_3axial_features = mean_vector +std_vector + mad_vector + max_vector + min_vector + [sma_value] + energy_vector + IQR_vector + entropy_vector + max_inds_vector + mean_Freq_vector + skewness_and_kurtosis_vector + bands_energy_vector\n",
    "\n",
    "        f_all_axial_features = f_all_axial_features+ f_3axial_features # add features to the global list\n",
    "        \n",
    "    return f_all_axial_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Frequency Magnitudes features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to generate frequency magnitude features\n",
    "\n",
    "# sma\n",
    "def f_sma_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    sma_value=float((abs(array)/math.sqrt(len(mag_column))).sum()) # sma of one mag f_signals\n",
    "    \n",
    "    return sma_value\n",
    "\n",
    "# energy\n",
    "def f_energy_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    # spectral energy value\n",
    "    energy_value=float((array**2).sum()/float(len(array))) # energy value of one mag f_signals\n",
    "    return energy_value\n",
    "\n",
    "\n",
    "####### Max Inds and Mean_Freq Functions#######################################\n",
    "\n",
    "\n",
    "# max_Inds\n",
    "def f_max_Inds_mag(mag_column,dfreq):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    \n",
    "    max_Inds_value =float(dfreq[array.argmax()]) # freq value related with max component\n",
    "    \n",
    "    return max_Inds_value\n",
    "\n",
    "# mean freq()\n",
    "def f_mean_Freq_mag(mag_column,dfreq):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    \n",
    "    mean_freq_value = float(np.dot(dfreq,array).sum() / float(array.sum())) # weighted sum of one mag f_signal\n",
    "    \n",
    "    return  mean_freq_value\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "########## Skewness & Kurtosis Functions #######################################\n",
    "\n",
    "from scipy.stats import skew           # skewness\n",
    "def f_skewness_mag(mag_column):\n",
    "    \n",
    "    array=np.array(mag_column)\n",
    "    skew_value     = float(skew(array)) # skewness value of one mag f_signal\n",
    "    return skew_value\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import kurtosis       # kurtosis\n",
    "def f_kurtosis_mag(mag_column):\n",
    "    array=np.array(mag_column)\n",
    "    kurtosis_value = float(kurtosis(array)) # kurotosis value of on mag f_signal\n",
    "\n",
    "    return kurtosis_value\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Frequency Magnitude features pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_mag_features_generation(f_window,dfreq):\n",
    "    \n",
    "    # select frequnecy mag columns : the last 2 columns in f_window\n",
    "    mag_columns=f_window.columns[-2:]\n",
    "    mag_columns=f_window[mag_columns]\n",
    "    \n",
    "    f_mag_features=[]\n",
    "    for col in mag_columns: # iterate throw each mag column in f_window\n",
    "        \n",
    "        # calculate common mag features and frequency mag features for each column\n",
    "        mean_value   = mean_mag(mag_columns[col])\n",
    "        std_value    = std_mag(mag_columns[col])\n",
    "        mad_value    = mad_mag(mag_columns[col])\n",
    "        max_value    = max_mag(mag_columns[col])\n",
    "        min_value    = min_mag(mag_columns[col])\n",
    "        sma_value    = f_sma_mag(mag_columns[col])\n",
    "        energy_value = f_energy_mag(mag_columns[col])\n",
    "        IQR_value    = IQR_mag(mag_columns[col])\n",
    "        entropy_value= entropy_mag(mag_columns[col])\n",
    "        max_Inds_value=f_max_Inds_mag(mag_columns[col],dfreq)\n",
    "        mean_Freq_value= f_mean_Freq_mag (mag_columns[col],dfreq)\n",
    "        skewness_value=  f_skewness_mag(mag_columns[col])\n",
    "        kurtosis_value = f_kurtosis_mag(mag_columns[col])\n",
    "        # 13 value per each t_mag_column\n",
    "        col_mag_values = [mean_value, std_value, mad_value, max_value, \n",
    "                          min_value, sma_value, energy_value,IQR_value, \n",
    "                          entropy_value, max_Inds_value, mean_Freq_value,\n",
    "                          skewness_value, kurtosis_value ]\n",
    "        \n",
    "        \n",
    "        f_mag_features= f_mag_features+ col_mag_values # append feature values of one mag column to the global list\n",
    "    \n",
    "    # f_mag_features contains 65 values = 13 value (per each t_mag_column) x 4 (f_mag_columns)\n",
    "    return f_mag_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency features name generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_features_names():\n",
    "    #Generating Frequency feature names\n",
    "    \n",
    "    # frequency axial signal names \n",
    "    axial_signals=[\n",
    "                    ['f_body_acc_X','f_body_acc_Y','f_body_acc_Z'],\n",
    "                    ['f_body_acc_Jerk_X','f_body_acc_Jerk_Y','f_body_acc_Jerk_Z']]\n",
    "\n",
    "    # frequency magnitude signals\n",
    "    mag_signals=['f_body_acc_Mag','f_body_acc_Jerk_Mag']\n",
    "\n",
    "\n",
    "    # features functions names will be applied to f_signals\n",
    "    f_one_input_features_name1=['_mean()','_std()','_mad()','_max()','_min()']\n",
    "\n",
    "    f_one_input_features_name2=['_energy()','_iqr()','_entropy()','_maxInd()','_meanFreq()']\n",
    "\n",
    "    f_one_input_features_name3= ['_skewness()','_kurtosis()']\n",
    "\n",
    "\n",
    "    f_one_input_features_name4=[\n",
    "                                '_BE[1-4]','_BE[4-7]','_BE[7-10]','_BE[10-13]',\n",
    "                                '_BE[13-16]','_BE[16-19]','_BE[19-22]','_BE[22-25]',\n",
    "                                '_BE[1-7]','_BE[7-13]','_BE[13-19]','_BE[19-25]',\n",
    "                                '_BE[1-12]','_BE[12-25]','_SE_B1','_SE_B2','_SE_B3'\n",
    "                               ]\n",
    "    \n",
    "    frequency_features_names=[] # global list of frequency features\n",
    "    \n",
    "    for columns in axial_signals: # iterate throw each group of 3-axial signals\n",
    "        \n",
    "        # iterate throw the first list of features\n",
    "        for feature in f_one_input_features_name1: \n",
    "            for column in columns:# iterate throw each signal name of that group\n",
    "                newcolumn=column[:-2]+feature+column[-2:] # build the full feature name\n",
    "                frequency_features_names.append(newcolumn) # add the feature name to the global list\n",
    "        \n",
    "        # sma feature name\n",
    "        sma_column=column[:-2]+'_sma()'\n",
    "        frequency_features_names.append(sma_column)\n",
    "\n",
    "        # iterate throw the first list of features\n",
    "        for feature in f_one_input_features_name2:\n",
    "            for column in columns:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                frequency_features_names.append(newcolumn)\n",
    "        \n",
    "        # iterate throw each signal name of that group\n",
    "        for column in columns:\n",
    "            for feature in f_one_input_features_name3: # iterate throw [skewness ,kurtosis]\n",
    "                newcolumn=column[:-2]+feature+column[-2:] # build full feature name\n",
    "                frequency_features_names.append(newcolumn) # append full feature names\n",
    "        \n",
    "        # same process above will be applied to list number 4\n",
    "        for column in columns:\n",
    "            for feature in f_one_input_features_name4:\n",
    "                newcolumn=column[:-2]+feature+column[-2:]\n",
    "                frequency_features_names.append(newcolumn)\n",
    "   \n",
    "    #################################################################################################################\n",
    "    # generate frequency mag features names\n",
    "    for column in mag_signals:# iterate throw each frequency mag signal name\n",
    "        for feature in f_one_input_features_name1:# iterate throw the first list of features functions names\n",
    "            frequency_features_names.append(column+feature) # build the full feature name and add it to the global list\n",
    "\n",
    "        sma_column=column+'_sma()' # build the sma full feature name\n",
    "        frequency_features_names.append(sma_column) # add it to the global list\n",
    "\n",
    "        for feature in f_one_input_features_name2:# iterate throw the second list of features functions names\n",
    "            frequency_features_names.append(column+feature)# build the full feature name and add it to the global list\n",
    "        \n",
    "        for feature in f_one_input_features_name3:# iterate throw the third list of features functions names\n",
    "            frequency_features_names.append(column+feature)# build the full feature name and add it to the global list\n",
    "    ####################################################################################################################\n",
    "    \n",
    "    return frequency_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Addtional features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Angles Functions ####################################\n",
    "from math import acos # inverse of cosinus function\n",
    "from math import sqrt # square root function\n",
    "\n",
    "########Euclidian magnitude 3D############\n",
    "def magnitude_vector(vector3D): # vector[X,Y,Z]\n",
    "    return sqrt((vector3D**2).sum()) # eulidian norm of that vector\n",
    "\n",
    "###########angle between two vectors in radian ###############\n",
    "def angle(vector1, vector2):\n",
    "    vector1_mag=magnitude_vector(vector1) # euclidian norm of V1\n",
    "    vector2_mag=magnitude_vector(vector2) # euclidian norm of V2\n",
    "   \n",
    "    scalar_product=np.dot(vector1,vector2) # scalar product of vector 1 and Vector 2\n",
    "    cos_angle=scalar_product/float(vector1_mag*vector2_mag) # the cosinus value of the angle between V1 and V2\n",
    "    \n",
    "    # just in case some values were added automatically\n",
    "    if cos_angle>1:\n",
    "        cos_angle=1\n",
    "    elif cos_angle<-1:\n",
    "        cos_angle=-1\n",
    "    \n",
    "    angle_value=float(acos(cos_angle)) # the angle value in radian\n",
    "    return angle_value # in radian.\n",
    "\n",
    "################## angle_features ############################\n",
    "def angle_features(t_window): # it returns 7 angles per window\n",
    "    angles_list=[]# global list of angles values\n",
    "    \n",
    "    # mean value of each column t_body_acc[X,Y,Z]\n",
    "    V2_columns=['t_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z']\n",
    "    V2_Vector=np.array(t_window[V2_columns].mean()) # mean values\n",
    "    \n",
    "    # angle 0: angle between (t_body_acc[X.mean,Y.mean,Z.mean], t_gravity[X.mean,Y.mean,Z.mean])\n",
    "    V1_columns=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z']\n",
    "    V1_Vector=np.array(t_window[V1_columns].mean()) # mean values of t_body_acc[X,Y,Z]\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector)) # angle between the vectors added to the global list\n",
    "    \n",
    "    # same process is applied to ither signals\n",
    "    # angle 1: (t_body_acc_jerk[X.mean,Y.mean,Z.mean],t_gravity[X.mean,Y.mean,Z.mean]\n",
    "    V1_columns=['t_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z']\n",
    "    V1_Vector=np.array(t_window[V1_columns].mean())\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    #################################################################################\n",
    "    \n",
    "    # V1 vector in this case is the X axis itself [1,0,0]\n",
    "    # angle 4: ([X_axis],t_gravity[X.mean,Y.mean,Z.mean])   \n",
    "    V1_Vector=np.array([1,0,0])\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    # V1 vector in this case is the Y axis itself [0,1,0]\n",
    "    # angle 5: ([Y_acc_axis],t_gravity[X.mean,Y.mean,Z.mean]) \n",
    "    V1_Vector=np.array([0,1,0])\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    # V1 vector in this case is the Z axis itself [0,0,1]\n",
    "    # angle 6: ([Z_acc_axis],t_gravity[X.mean,Y.mean,Z.mean])\n",
    "    V1_Vector=np.array([0,0,1])\n",
    "    angles_list.append(angle(V1_Vector, V2_Vector))\n",
    "    \n",
    "    return angles_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Additional features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_columns=['angle0()','angle1()','angle2()','angle3()','angle4()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Datasets generation PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conctenate all features names lists and we add two other columns activity ids and user ids will be related to each row\n",
    "all_columns=time_features_names()+frequency_features_names()+angle_columns\n",
    "\n",
    "def Dataset_Generation_PipeLine(b):\n",
    "    data,sampling_freq=preprocess_real_smartphone(pd.read_csv(b))\n",
    "    time_sig_df=time_domain_signal(data,sampling_freq)\n",
    "    freq_sig_df,dfreq=fast_fourier_transform(time_sig_df,sampling_freq)\n",
    "\n",
    "    # conctenate all features names lists and we add two other columns activity ids and user ids will be related to each row\n",
    "    all_columns=time_features_names()+frequency_features_names()+angle_columns\n",
    "    # generate all time features from t_window \n",
    "    time_features = t_axial_features_generation(time_sig_df) + t_mag_features_generation(time_sig_df)\n",
    "    # generate all frequency features from f_window\n",
    "    frequency_features = f_axial_features_generation(freq_sig_df,dfreq) + f_mag_features_generation(freq_sig_df,dfreq)\n",
    "\n",
    "    # Generate addtional features from t_window\n",
    "    additional_features= angle_features(time_sig_df)\n",
    "\n",
    "    # concatenate all features and append the activity id and the user id\n",
    "    row= time_features + frequency_features + additional_features \n",
    "    return(row)\n",
    "\n",
    "def Dataset_Generation_PipeLine_SmartWatch(b):\n",
    "    data,device_id,sampling_freq=preprocess_real_smartwatch(pd.read_csv(b))\n",
    "    time_sig_df=time_domain_signal(data,sampling_freq)\n",
    "    freq_sig_df,dfreq=fast_fourier_transform(time_sig_df,sampling_freq)\n",
    "\n",
    "    # conctenate all features names lists and we add two other columns activity ids and user ids will be related to each row\n",
    "    all_columns=time_features_names()+frequency_features_names()+angle_columns\n",
    "    # generate all time features from t_window \n",
    "    time_features = t_axial_features_generation(time_sig_df) + t_mag_features_generation(time_sig_df)\n",
    "    # generate all frequency features from f_window\n",
    "    frequency_features = f_axial_features_generation(freq_sig_df,dfreq) + f_mag_features_generation(freq_sig_df,dfreq)\n",
    "\n",
    "    # Generate addtional features from t_window\n",
    "    additional_features= angle_features(time_sig_df)\n",
    "\n",
    "    # concatenate all features and append the activity id and the user id\n",
    "    row= [device_id]+time_features + frequency_features + additional_features \n",
    "    return(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmartPhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "a_real=glob.glob(\"training_data/smartphone_accelerometer/*.csv\")\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine)(i) for i in a_real)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#3.4828 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(result)\n",
    "df_train.columns=all_columns\n",
    "df_train['measurement_id']=[item[len('training_data/smartphone_accelerometer/'):-4] for item in a_real]\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ancillary_data\n",
    "b_real=glob.glob(\"ancillary_data/smartphone_accelerometer/*.csv\")\n",
    "b_real2=[]\n",
    "for i in b_real:\n",
    "    a=pd.read_csv(i).shape[0]\n",
    "    if a>=2000:\n",
    "        b_real2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine)(i) for i in b_real2)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#3.2621 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ancillary=pd.DataFrame(result)\n",
    "df_ancillary.columns=all_columns\n",
    "df_ancillary['measurement_id']=[item[len('ancillary_data/smartphone_accelerometer/'):-4] for item in b_real2]\n",
    "print(df_ancillary.shape)\n",
    "df_ancillary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame = df_train.append(pd.DataFrame(data = df_ancillary), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export part1 features of training data from smartphone signal for realpd\n",
    "Frame.to_csv('analysis2_realpd_comp_training_abhiroop_tillhurst_smartphone.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "a_real=glob.glob(\"testing_data/smartphone_accelerometer/*.csv\")\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine)(i) for i in a_real)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#1.1097 Mins\n",
    "\n",
    "df_train=pd.DataFrame(result)\n",
    "df_train.columns=all_columns\n",
    "df_train['measurement_id']=[item[len('testing_data/smartphone_accelerometer/'):-4] for item in a_real]\n",
    "print(df_train.shape)\n",
    "Frame_test=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export part1 features of testing data from smartphone signal for realpd\n",
    "Frame_test.to_csv('analysis2_realpd_comp_testing_abhiroop_tillhurst_smartphone.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmartWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "a_real=glob.glob(\"training_data/smartwatch_accelerometer/*.csv\")\n",
    "a_real_acc=[]\n",
    "num=[]\n",
    "for i in a_real:\n",
    "    data=pd.read_csv(i)\n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    data=data[data.device_id==deviceid].reset_index(drop=True)\n",
    "    num.append(data.shape[0])\n",
    "    if data.shape[0]>=2000:\n",
    "        a_real_acc.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# smartwatch_accelerometer\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine_SmartWatch)(i) for i in a_real_acc)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#1.8061357021331788 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(result)\n",
    "df_train.columns=['device_id_acc']+all_columns\n",
    "df_train['measurement_id']=[item[len('training_data/smartwatch_accelerometer/'):-4] for item in a_real_acc]\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ancillary\n",
    "a_real=glob.glob(\"ancillary_data/smartwatch_accelerometer/*.csv\")\n",
    "a_real_acc=[]\n",
    "num=[]\n",
    "for i in a_real:\n",
    "    data=pd.read_csv(i)\n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    data=data[data.device_id==deviceid].reset_index(drop=True)\n",
    "    num.append(data.shape[0])\n",
    "    if data.shape[0]>=2000:\n",
    "        a_real_acc.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smartwatch_accelerometer\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine_SmartWatch)(i) for i in a_real_acc)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#1.8061357021331788 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ancillary=pd.DataFrame(result)\n",
    "df_ancillary.columns=['device_id_acc']+all_columns\n",
    "df_ancillary['measurement_id']=[item[len('ancillary_data/smartwatch_accelerometer/'):-4] for item in a_real_acc]\n",
    "print(df_ancillary.shape)\n",
    "df_ancillary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame_smartwatch_acc = df_train.append(pd.DataFrame(data = df_ancillary), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "a_real=glob.glob(\"testing_data/smartwatch_accelerometer/*.csv\")\n",
    "a_real_acc=[]\n",
    "num=[]\n",
    "for i in a_real:\n",
    "    data=pd.read_csv(i)\n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    data=data[data.device_id==deviceid].reset_index(drop=True)\n",
    "    num.append(data.shape[0])\n",
    "    if data.shape[0]>=2000:\n",
    "        a_real_acc.append(i)\n",
    "\n",
    "print(len(a_real_acc),len(a_real))\n",
    "\n",
    "# smartwatch_accelerometer\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine_SmartWatch)(i) for i in a_real_acc)\n",
    "\n",
    "\n",
    "df_train=pd.DataFrame(result)\n",
    "df_train.columns=['device_id_acc']+all_columns\n",
    "df_train['measurement_id']=[item[len('testing_data/smartwatch_accelerometer/'):-4] for item in a_real_acc]\n",
    "print(df_train.shape)\n",
    "\n",
    "Frame_smartwatch_acc_test=df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gyroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_real_gyroscope(data):\n",
    "    \n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    \n",
    "    data=data[data.device_id==deviceid].reset_index()\n",
    "    data.rename(columns={'t':'Timestamp','x':'X','y':'Y','z':'Z'},inplace=True)\n",
    "   \n",
    "    ls=['X','Y','Z']\n",
    "    #freq=round((1/((data.Timestamp.max()/data.Timestamp.shape[0]).round(3))),0)\n",
    "    freq=50\n",
    "    t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],0.02)\n",
    "    #t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],(data.Timestamp.max()/data.Timestamp.shape[0]).round(3))\n",
    "    df=pd.DataFrame({'Timestamp':t1})\n",
    "        \n",
    "    for i in ls:\n",
    "        fcubic = interpolate.interp1d(data.Timestamp, data[i])\n",
    "        df[i]=fcubic(t1)\n",
    "    df.rename(columns={'X':'acc_X','Y':'acc_Y','Z':'acc_Z'},inplace=True)\n",
    "    return df[['Timestamp','acc_X','acc_Y','acc_Z']],deviceid,freq\n",
    "\n",
    "\n",
    "def Dataset_Generation_PipeLine_gyroscope(b):\n",
    "    data,device_id,sampling_freq=preprocess_real_gyroscope(pd.read_csv(b))\n",
    "    time_sig_df=time_domain_signal(data,sampling_freq)\n",
    "    freq_sig_df,dfreq=fast_fourier_transform(time_sig_df,sampling_freq)\n",
    "\n",
    "    # conctenate all features names lists and we add two other columns activity ids and user ids will be related to each row\n",
    "    all_columns=time_features_names()+frequency_features_names()+angle_columns\n",
    "    # generate all time features from t_window \n",
    "    time_features = t_axial_features_generation(time_sig_df) + t_mag_features_generation(time_sig_df)\n",
    "    # generate all frequency features from f_window\n",
    "    frequency_features = f_axial_features_generation(freq_sig_df,dfreq) + f_mag_features_generation(freq_sig_df,dfreq)\n",
    "\n",
    "    # Generate addtional features from t_window\n",
    "    additional_features= angle_features(time_sig_df)\n",
    "\n",
    "    # concatenate all features and append the activity id and the user id\n",
    "    row= [device_id]+time_features + frequency_features + additional_features \n",
    "    return(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "a_real=glob.glob(\"training_data/smartwatch_gyroscope/*.csv\")\n",
    "a_real_gyro=[]\n",
    "num=[]\n",
    "for i in a_real:\n",
    "    data=pd.read_csv(i)\n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    data=data[data.device_id==deviceid].reset_index(drop=True)\n",
    "    num.append(data.shape[0])\n",
    "    if data.shape[0]>=2000:\n",
    "        a_real_gyro.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smartwatch_accelerometer\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine_gyroscope)(i) for i in a_real_gyro)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#1.8061357021331788 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(result)\n",
    "df_train.columns=['device_id_gyro']+[i.replace('acc','gyro') for i in all_columns]\n",
    "df_train['measurement_id']=[item[len('training_data/smartwatch_gyroscope/'):-4] for item in a_real_gyro]\n",
    "df_train=df_train[[i for i in list(df_train.columns) if not any(w in 'grav' for w in i.split('_'))]]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_real=glob.glob(\"ancillary_data/smartwatch_gyroscope/*.csv\")\n",
    "a_real_gyro=[]\n",
    "num=[]\n",
    "for i in a_real:\n",
    "    data=pd.read_csv(i)\n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    data=data[data.device_id==deviceid].reset_index(drop=True)\n",
    "    num.append(data.shape[0])\n",
    "    if data.shape[0]>=2000:\n",
    "        a_real_gyro.append(i)\n",
    "#pd.DataFrame({'num':num}).sort_values(by='num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smartwatch_accelerometer\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine_gyroscope)(i) for i in a_real_gyro)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#1.8061357021331788 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ancillary=pd.DataFrame(result)\n",
    "df_ancillary.columns=['device_id_gyro']+[i.replace('acc','gyro') for i in all_columns]\n",
    "df_ancillary['measurement_id']=[item[len('ancillary_data/smartwatch_gyroscope/'):-4] for item in a_real_gyro]\n",
    "df_ancillary=df_ancillary[[i for i in list(df_ancillary.columns) if not any(w in 'grav' for w in i.split('_'))]]\n",
    "#df_ancillary=df_ancillary.drop('device_id_gyro',axis=1)\n",
    "print(df_ancillary.shape)\n",
    "df_ancillary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame_smartwatch_gyro = df_train.append(pd.DataFrame(data = df_ancillary), ignore_index=True)\n",
    "print(Frame_smartwatch_gyro.shape)\n",
    "Frame_smartwatch_gyro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "a_real=glob.glob(\"testing_data/smartwatch_gyroscope/*.csv\")\n",
    "a_real_gyro=[]\n",
    "num=[]\n",
    "for i in a_real:\n",
    "    data=pd.read_csv(i)\n",
    "    a=data.groupby('device_id').agg({'x':'var','y':'count'}).reset_index()\n",
    "    deviceid=a.loc[a.x.idxmax(),'device_id']\n",
    "    if int(a.loc[a.device_id==deviceid,'y'])<=data.shape[0]*0.2:\n",
    "        deviceid=a.loc[a.x.idxmin(),'device_id']\n",
    "    data=data[data.device_id==deviceid].reset_index(drop=True)\n",
    "    num.append(data.shape[0])\n",
    "    if data.shape[0]>=2000:\n",
    "        a_real_gyro.append(i)\n",
    "\n",
    "# smartwatch_accelerometer\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine_gyroscope)(i) for i in a_real_gyro)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "df_train=pd.DataFrame(result)\n",
    "df_train.columns=['device_id_gyro']+[i.replace('acc','gyro') for i in all_columns]\n",
    "df_train['measurement_id']=[item[len('testing_data/smartwatch_gyroscope/'):-4] for item in a_real_gyro]\n",
    "df_train=df_train[[i for i in list(df_train.columns) if not any(w in 'grav' for w in i.split('_'))]]\n",
    "print(df_train.shape)\n",
    "\n",
    "Frame_smartwatch_gyro_test=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging smartwatch_accelerometer and gyroscope for training and testing data\n",
    "Frame_smartwatch=pd.merge(Frame_smartwatch_acc,Frame_smartwatch_gyro,on='measurement_id')\n",
    "Frame_smartwatch=Frame_smartwatch.drop('device_id_gyro',axis=1)\n",
    "\n",
    "Frame_smartwatch_test=pd.merge(Frame_smartwatch_acc_test,Frame_smartwatch_gyro_test,on='measurement_id')\n",
    "Frame_smartwatch_test=Frame_smartwatch_test.drop('device_id_gyro',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export part1 features of training data from smartwatch signal for realpd\n",
    "Frame_smartwatch.to_csv('analysis2_realpd_comp_training_abhiroop_tillhurst_smartwatch.csv',index=False)\n",
    "\n",
    "#export part1 features of testing data from smartwatch signal for realpd\n",
    "Frame_smartwatch_test.to_csv('analysis2_realpd_comp_testing_abhiroop_tillhurst_smartwatch.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

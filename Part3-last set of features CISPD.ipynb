{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import glob\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "import heapq\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import pywt\n",
    "from numpy.fft import fft\n",
    "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum, ones, log2, std\n",
    "from numpy.linalg import svd, lstsq\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, FeaturesData, Pool\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================================\n",
    "#making time stamp uniform by Interpolation\n",
    "\n",
    "def preprocess(data):\n",
    "    freq=50\n",
    "    ls=['X','Y','Z']\n",
    "    t1=np.arange(data.Timestamp[0],data.Timestamp[(data.shape[0])-1],0.02)\n",
    "    df=pd.DataFrame({'Timestamp':t1})\n",
    "    for i in ls:\n",
    "        fcubic = interpolate.interp1d(data.Timestamp, data[i], kind='cubic')\n",
    "        df[i]=fcubic(t1)\n",
    "    df.columns=['Timestamp','acc_X','acc_Y','acc_Z']\n",
    "    return df\n",
    "\n",
    "#==========================================================================================\n",
    "#median filter\n",
    "from scipy.signal import medfilt # import the median filter function\n",
    "def median(signal):# input: numpy array 1D (one column)  \n",
    "    #applying the median filter\n",
    "    return  medfilt(np.array(signal), kernel_size=3) # applying the median filter order3(kernel_size=3)\n",
    "\n",
    "\n",
    "#==========================================================================================\n",
    "#components_selection_one_signal\n",
    "import math # import math library\n",
    "\n",
    "\n",
    "def components_selection_one_signal(t_signal):\n",
    "    sampling_freq=50\n",
    "    nyq=sampling_freq/float(2) # nyq is the nyquist frequency equal to the half of the sampling frequency[50/2= 25 Hz]\n",
    "\n",
    "    freq1 = 0.3\n",
    "    freq2 = 20\n",
    "\n",
    "    t_signal=np.array(t_signal)\n",
    "    t_signal_length=len(t_signal) # number of points in a t_signal\n",
    "    \n",
    "    # the t_signal in frequency domain after applying fft\n",
    "    f_signal=np.fft.fft(t_signal) # 1D numpy array contains complex values (in C)\n",
    "    \n",
    "    # generate frequencies associated to f_signal complex values\n",
    "    freqs=np.array(np.fft.fftfreq(t_signal_length, d=1/float(sampling_freq))) # frequency values between [-25hz:+25hz]\n",
    "        \n",
    "    df=pd.DataFrame({'freq':abs(freqs),'amplitute':f_signal})\n",
    "    df['f_DC_signal']=np.where(df.freq>freq1,0,df.amplitute)\n",
    "    df['f_noise_signal']=np.where(df.freq<=freq2,0,df.amplitute)\n",
    "    df['f_body_signal']=np.where(df.freq<=freq1,0,np.where(df.freq>freq2,0,df.amplitute))\n",
    "\n",
    "    \n",
    "    # Inverse the transformation of signals in freq domain #\n",
    "    # applying the inverse fft(ifft) to signals in freq domain and put them in float format\n",
    "    t_DC_component= np.fft.ifft(np.array(df['f_DC_signal'])).real\n",
    "    t_body_component= np.fft.ifft(np.array(df['f_body_signal'])).real\n",
    "    t_noise=np.fft.ifft(np.array(df['f_noise_signal'])).real\n",
    "    \n",
    "    total_component=t_signal-t_noise # extracting the total component(filtered from noise) \n",
    "                                     #  by substracting noise from t_signal (the original signal).\n",
    "    \n",
    "    # return outputs mentioned earlier\n",
    "    return (total_component,t_DC_component,t_body_component,t_noise) \n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "#Define verify gravity function\n",
    "def mag_3_signals(df): # Euclidian magnitude\n",
    "    return np.array(np.sqrt(np.square(df).sum(axis=1)))\n",
    "\n",
    "def verify_gravity(data):\n",
    "    \n",
    "    acc_x=np.array(data['acc_X']) # copy acc_X column from dataframe in raw_dic having the key mentioned above\n",
    "    acc_y=np.array(data['acc_Y'])# copy acc_Y column  from dataframe in raw_dic having the key mentioned above\n",
    "    acc_z=np.array(data['acc_Z'])# copy acc_Z column  from dataframe in raw_dic having the key mentioned above\n",
    "\n",
    "    # apply the filtering method to acc_[X,Y,Z] and store gravity components\n",
    "    grav_acc_X=components_selection_one_signal(acc_x)[1] \n",
    "    grav_acc_Y=components_selection_one_signal(acc_y)[1]\n",
    "    grav_acc_Z=components_selection_one_signal(acc_z)[1]\n",
    "    \n",
    "    # calculating gravity magnitude signal\n",
    "    grav_acc_mag=mag_3_signals(grav_acc_X, grav_acc_Y,grav_acc_Z)\n",
    "    print('mean value = ',round((sum(grav_acc_mag) / len(grav_acc_mag)),3),' g')\n",
    "    \n",
    "#=================================================================================================================    \n",
    "#Define jerking and magnitude functions\n",
    "def jerk_one_signal(signal):\n",
    "    signal=pd.DataFrame(signal)\n",
    "    jerk=(signal.shift(-1)-signal)/0.02\n",
    "    return np.array(jerk.dropna()).transpose()[0]\n",
    "\n",
    "def time_domain_signal(data):\n",
    "    time_sig_df=pd.DataFrame()\n",
    "    for column in ['acc_X','acc_Y','acc_Z']:\n",
    "        t_signal=np.array(data[column])\n",
    "        #med_filtred=median(t_signal)\n",
    "        med_filtred=(t_signal)\n",
    "        _,grav_acc,body_acc,_=components_selection_one_signal(med_filtred)\n",
    "        body_acc_jerk=jerk_one_signal(body_acc)\n",
    "        time_sig_df['t_body_'+column]=body_acc[:-1]\n",
    "        time_sig_df['t_grav_'+column]= grav_acc[:-1]\n",
    "        time_sig_df['t_body_acc_jerk_'+column[-1]]=body_acc_jerk\n",
    "\n",
    "    # all 15 axial signals generated above are reordered to facilitate magnitudes signals generation\n",
    "    new_columns_ordered=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n",
    "                              't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n",
    "                              't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z']\n",
    "\n",
    "\n",
    "    # create new dataframe to order columns\n",
    "    time_sig_df=time_sig_df[new_columns_ordered]\n",
    "\n",
    "    # Magnitude Features\n",
    "    for i in range(0,9,3):\n",
    "        mag_col_name=new_columns_ordered[i][:-1]+'mag'# Create the magnitude column name related to each 3-axial signals\n",
    "        time_sig_df[mag_col_name]=mag_3_signals(time_sig_df[new_columns_ordered[i:i+3]]) # store the signal_mag with its appropriate column name\n",
    "\n",
    "    return(time_sig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fractal Dimension using Katz FD algorithm\n",
    "def katz(data):\n",
    "    n = len(data)-1\n",
    "    L = np.hypot(np.diff(data), 1).sum() # Sum of distances\n",
    "    d = np.hypot(data - data[0], np.arange(len(data))).max() # furthest distance from first point\n",
    "    return np.log10(n) / (np.log10(d/L) + np.log10(n))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Coefficient of variation\n",
    "def coeff_var(a):\n",
    "    output = np.std(a)/np.mean(a) #computing coefficient of variation\n",
    "    return output\n",
    "\n",
    "#Mean and variance of Vertex to Vertex Slope\n",
    "def slope(x):\n",
    "    \n",
    "    amp_max = np.array(x[argrelextrema(np.array(x), np.greater)[0]])\n",
    "    t_max = argrelextrema(np.array(x), np.greater)[0]\n",
    "    amp_min = np.array(x[argrelextrema(np.array(x), np.less)[0]])\n",
    "    t_min = argrelextrema(np.array(x), np.less)[0]\n",
    "    t = np.concatenate((t_max,t_min),axis=0)\n",
    "    t.sort()#sort on the basis of time\n",
    "\n",
    "    amp = np.zeros(len(t))\n",
    "    res = np.zeros(len(t))\n",
    "    \n",
    "    for l in range(len(t)):\n",
    "        amp[l]=x[t[l]]\n",
    "\n",
    "    amp_diff = first_diff(amp)\n",
    "    t_diff = first_diff(t)\n",
    "\n",
    "    for q in range(len(amp_diff)):\n",
    "        res[q] = amp_diff[q]/t_diff[q]         \n",
    "    \n",
    "    res=res[~np.isnan(res)]\n",
    "    return [np.mean(res),np.std(res)] #returning mean and std of vertex to vertex slope\n",
    "\n",
    "\n",
    "#Hjorth Parameter\n",
    "def hjorth(input):                                              \n",
    "    hfeatures = []\n",
    "    diff_input = np.diff(input)\n",
    "    diff_diffinput = np.diff(diff_input)\n",
    "    \n",
    "    hjorth_activity = np.var(input)\n",
    "    hjorth_mobility = np.sqrt(np.var(diff_input)/hjorth_activity)\n",
    "    hjorth_diffmobility = np.sqrt(np.var(diff_diffinput)/np.var(diff_input))\n",
    "    hjorth_complexity = hjorth_diffmobility/hjorth_mobility\n",
    "    \n",
    "    hfeatures.append(hjorth_activity)\n",
    "    hfeatures.append(hjorth_mobility)\n",
    "    hfeatures.append(hjorth_complexity)\n",
    "    \n",
    "    return hfeatures  #returning hjorth activity, hjorth mobility , hjorth complexity\n",
    "\n",
    "\n",
    "#Kurtosis\n",
    "def kurtosis(a):\n",
    "    mean_i = np.mean(a) # Saving the mean of array i\n",
    "    std_i = np.std(a) # Saving the standard deviation of array i\n",
    "    t = 0.0\n",
    "    for j in a:\n",
    "        t += (pow((j-mean_i)/std_i,4)-3)\n",
    "    kurtosis_i = t/len(a) # Formula: (1/N)*(summation(x_i-mean)/standard_deviation)^4-3\n",
    "    return kurtosis_i\n",
    "\n",
    "\n",
    "#Second difference Mean,Max,std\n",
    "def sec_diff(b):\n",
    "    temp1 = abs(b-b.shift(1)) # Obtaining the 1st Diffs\n",
    "    t = abs(temp1-temp1.shift(1)) # Summing the 2nd Diffs\n",
    "    output = t.mean() # Calculating the mean of the 2nd Diffs\n",
    "    return [t.mean(),t.max(),t.std()]\n",
    "\n",
    "#Skewness\n",
    "def skewness(a):\n",
    "    import scipy.stats as sp\n",
    "    skew_array=sp.stats.skew(a,axis=0,bias=True)\n",
    "    return skew_array #returning skewness\n",
    "\n",
    "\n",
    "#First Difference Mean,Max,std\n",
    "def first_diff_mean(a):\n",
    "    output = abs(a-a.shift(1)) # Obtaining the 1st Diffs\n",
    "    return [output.mean(),output.max(),output.std()] #returns first diff mean,max,min\n",
    "\n",
    "#First Difference\n",
    "def first_diff(a):\n",
    "    if str(type(a))!=\"<class 'pandas.core.series.Series'>\":\n",
    "        a=pd.DataFrame(a)[0]\n",
    "    output = a-a.shift(1)# Obtaining the 1st Diffs\n",
    "    return output\n",
    "\n",
    "\n",
    "#wavelet features\n",
    "def wavelet_features(epoch): \n",
    "    wfeatures = []\n",
    "    \n",
    "    #calculating the coefficients of wavelet transform.\n",
    "    cA_values,cD_values=pywt.dwt(epoch,'coif1')\n",
    "    \n",
    "    cA_square=np.array([x for x in (np.square(cA_values)).tolist() if x != 0])\n",
    "    cD_square=np.array([x for x in (np.square(cD_values)).tolist() if x != 0])\n",
    "    \n",
    "      \n",
    "    wfeatures.append(np.mean(cA_values)) #cA_mean\n",
    "    wfeatures.append(abs(np.std(cA_values))) #cA_std\n",
    "    wfeatures.append(abs(np.sum(np.square(cA_values)))) #cA_Energy\n",
    "    wfeatures.append(abs(np.sum(cA_square * np.log(cA_square)))) #Entropy_A\n",
    "    \n",
    "    wfeatures.append(np.mean(cD_values)) #cD_mean\n",
    "    wfeatures.append(abs(np.std(cD_values))) #cD_std\n",
    "    wfeatures.append(abs(np.sum(np.square(cD_values)))) #cD_Energy\n",
    "    wfeatures.append(abs(np.sum(cD_square * np.log(cD_square)))) #Entropy_D\n",
    "\n",
    "    return wfeatures # returning 'Wavelet Approximate Mean','Wavelet Approximate Std Deviation','Wavelet Approximate Energy','Wavelet Approximate Entropy','Wavelet Detailed Mean','Wavelet Detailed Std Deviation','Wavelet Detailed Energy','Wavelet Detailed Entropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_generation(t_window):\n",
    "    \n",
    "    # select mag columns : the last 3 columns in a time domain window\n",
    "    \n",
    "    mag_columns=t_window.columns # mag columns' names\n",
    "    \n",
    "    t_mag_features=[] # a global list will contain all time domain magnitude features\n",
    "    \n",
    "    for col in mag_columns: # iterate throw each mag column\n",
    "        \n",
    "        fkatz = [katz(t_window[col])] # 1 value\n",
    "        fcoeff_var   = [coeff_var(t_window[col])] # 1 value\n",
    "        fslope    = slope(t_window[col])# 2 value\n",
    "        fhjorth    = hjorth(t_window[col])# 3 value\n",
    "        fsec_diff    = sec_diff(t_window[col])# 3 value\n",
    "        ffirst_diff_mean = first_diff_mean(t_window[col])# 3 value\n",
    "        fwavelet_features    = wavelet_features(t_window[col])# 8 value\n",
    "        \n",
    "        # 13 value per each t_mag_column\n",
    "        col_mag_values = fkatz+fcoeff_var+fslope+fhjorth+fsec_diff+ffirst_diff_mean+fwavelet_features\n",
    "        \n",
    "        # col_mag_values will be added to the global list\n",
    "        t_mag_features= t_mag_features+ col_mag_values\n",
    "    \n",
    "    # t_mag_features contains 65 values = 13 values (per each t_mag_column) x 5 (t_mag_columns)\n",
    "    return t_mag_features\n",
    " \n",
    "def Dataset_Generation_PipeLine(b):\n",
    "    data=preprocess(pd.read_csv(b))\n",
    "    time_sig_df=time_domain_signal(data)\n",
    "    # concatenate all features and append the activity id and the user id\n",
    "    row= features_generation(time_sig_df)\n",
    "    return(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_names():\n",
    "    # Generating time feature names\n",
    "    \n",
    "    # time domain magnitude signals' names\n",
    "    magnitude_signals=['t_body_acc_X','t_body_acc_Y','t_body_acc_Z',\n",
    "                       't_grav_acc_X','t_grav_acc_Y','t_grav_acc_Z',\n",
    "                       't_body_acc_jerk_X','t_body_acc_jerk_Y','t_body_acc_jerk_Z',\n",
    "                       't_body_acc_Mag','t_grav_acc_Mag','t_body_acc_jerk_Mag']\n",
    "\n",
    "    # functions' names:\n",
    "    t_one_input_features_name1=['_katz()','_coeff_var()']\n",
    "\n",
    "    t_one_input_features_slope=['_slope_mean()','_slope_std()']\n",
    "\n",
    "    t_one_input_features_hjorth=['_hjorth_activity()','_hjorth_mobility()','_hjorth_complexity()']\n",
    "\n",
    "    t_one_input_features_sec_diff=['_sec_diff_mean()','_sec_diff_max()','_sec_diff_std()']\n",
    "    t_one_input_features_first_diff=['_first_diff_mean()','_first_diff_max()','_first_diff_std()']\n",
    "    \n",
    "    t_one_input_features_wavelet=['_wavelet_cA_mean()','_wavelet_cA_std()','_wavelet_cA_Energy()','_wavelet_Entropy_A()',\n",
    "                                  '_wavelet_cD_mean()','_wavelet_cD_std()','_wavelet_cD_Energy()','_wavelet_Entropy_D()']\n",
    "    \n",
    "    features=[]# Empty list : it will contain all time domain features' names\n",
    "    \n",
    "    for columns in magnitude_signals: # iterate throw time domain magnitude column names\n",
    "\n",
    "        # build feature names related to that column\n",
    "        #list 1\n",
    "        for feature in t_one_input_features_name1:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "        \n",
    "        # list 2\n",
    "        for feature in t_one_input_features_slope: \n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "            \n",
    "       \n",
    "        # list 3\n",
    "        for feature in t_one_input_features_hjorth:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "            \n",
    "        # list 4\n",
    "        for feature in t_one_input_features_sec_diff:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "        # list 5\n",
    "        for feature in t_one_input_features_first_diff:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "        # list 6\n",
    "        for feature in t_one_input_features_wavelet:\n",
    "            newcolumn=columns+feature\n",
    "            features.append(newcolumn)\n",
    "            \n",
    "        \n",
    "    ###########################################################################################################\n",
    "    time_list_features=features\n",
    "    \n",
    "    return time_list_features # return all time domain features' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "a=glob.glob(\"training_data/*.csv\")\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine)(i) for i in a)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#47.693 min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=pd.DataFrame(result)\n",
    "training_data.columns=features_names()\n",
    "training_data['measurement_id']=[item[len('training_data/'):-4] for item in a]\n",
    "print(training_data.shape) #(1858, 253)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ancillary\n",
    "b=glob.glob(\"ancillary_data/*.csv\")\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine)(i) for i in b)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#12.124 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_data=pd.DataFrame(result)\n",
    "ancillary_data.columns=features_names()\n",
    "ancillary_data['measurement_id']=[item[len('ancillary_data/'):-4] for item in b]\n",
    "print(ancillary_data.shape)\n",
    "ancillary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging training and ancillary_data\n",
    "data=training_data.append(pd.DataFrame(ancillary_data),ignore_index=True)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export part3 features of training data for cispd\n",
    "data.to_csv('cispd_comp_training_abhiroop_lastfeatures.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "a=glob.glob(\"testing_data/*.csv\")\n",
    "start_time = time.time()\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "result=Parallel(n_jobs=num_cores)(delayed(Dataset_Generation_PipeLine)(i) for i in a)\n",
    "print(\"--- %s Mins ---\" % ((time.time() - start_time)/60))\n",
    "#18.773 min \n",
    "\n",
    "testing_data=pd.DataFrame(result)\n",
    "testing_data.columns=features_names()\n",
    "testing_data['measurement_id']=[item[len('testing_data/'):-4] for item in a]\n",
    "print(testing_data.shape) #(1858, 253)\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export part3 features of testing data for cispd\n",
    "testing_data.to_csv('cispd_comp_testing_abhiroop_lastfeatures.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIS-PD_Demographics\n",
    "cis_pd_demo=pd.read_csv('clinical_data/CIS-PD_Demographics.csv')\n",
    "cis_pd_demo=cis_pd_demo.drop(['Race','Ethnicity'],axis=1)\n",
    "cis_pd_demo['Gender']=cis_pd_demo['Gender'].map({'Male':1,'Female':0})\n",
    "print(cis_pd_demo.shape)\n",
    "cis_pd_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIS-PD_UPDRS_Part1_2_4\n",
    "cis_pd_updrs_part1_2_4=pd.read_csv('clinical_data/CIS-PD_UPDRS_Part1_2_4.csv')\n",
    "cis_pd_updrs_part1_2_4.drop('Visit',axis=1,inplace=True)\n",
    "print(cis_pd_updrs_part1_2_4.shape)\n",
    "cis_pd_updrs_part1_2_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CIS-PD_UPDRS_Part3\n",
    "cis_pd_updrs_part3=pd.read_csv('clinical_data/CIS-PD_UPDRS_Part3.csv')\n",
    "cis_pd_updrs_part3.drop(['UPDRS_3a','UPDRS_3b','UPDRS_3c','UPDRS_3C1','ParticipantState','UPDRS_3.19B'],axis=1,inplace=True)\n",
    "\n",
    "#Cleaning and feature creation. from CIS-PD_UPDRS_Part3\n",
    "visit1=cis_pd_updrs_part3.Visit.map({'Baseline':0,'2 Weeks: Time 0':1,'2 Weeks: Time 60':1})\n",
    "visit2=cis_pd_updrs_part3.Visit.map({'Baseline':0,'2 Weeks: Time 0':0,'2 Weeks: Time 60':1})\n",
    "\n",
    "col=[e for e in cis_pd_updrs_part3.columns if e not in ('subject_id','Visit')]\n",
    "cis_pd_updrs_part3=cis_pd_updrs_part3.groupby(['subject_id'])[col].ffill().bfill()\n",
    "\n",
    "cis_pd_updrs_part3['Visit']=visit2\n",
    "cis_pd_updrs_part3['Visit_Type']=visit1\n",
    "cis_pd_updrs_part3['UPDRS_3.19A']=cis_pd_updrs_part3['UPDRS_3.19A'].map({'Yes':1,'No':0})\n",
    "\n",
    "cis_pd_updrs_part3['Rigidity']=cis_pd_updrs_part3[['UPDRS_3.3 Neck','UPDRS_3.3 Right Upper Extremity','UPDRS_3.3 Left Upper Extremity','UPDRS_3.3 Right Lower Extremity','UPDRS_3.3 Left Lower Extremity']].mean(axis=1)\n",
    "cis_pd_updrs_part3['Tapping']=cis_pd_updrs_part3[['UPDRS_3.4 Right Hand','UPDRS_3.4 Left Hand','UPDRS_3.7 Right Foot','UPDRS_3.7 Left Foot']].mean(axis=1)\n",
    "cis_pd_updrs_part3['Body Tremor']=cis_pd_updrs_part3[['UPDRS_3.15 Right Hand','UPDRS_3.15 Left Hand','UPDRS_3.16 Right Hand','UPDRS_3.16 Left Hand','UPDRS_3.17 Right Upper Extremity','UPDRS_3.17 Left Upper Extremity','UPDRS_3.17 Right Lower Extremity','UPDRS_3.17 Left Lower Extremity','UPDRS_3.17 Lip-Jaw']].mean(axis=1)\n",
    "cis_pd_updrs_part3['Movement']=cis_pd_updrs_part3[['UPDRS_3.5 Right Hand','UPDRS_3.5 Left Hand','UPDRS_3.6 Right Hand','UPDRS_3.6 Left Hand']].mean(axis=1)\n",
    "\n",
    "type1=cis_pd_updrs_part3.copy()\n",
    "type1.drop(['Visit','Visit_Type'],axis=1,inplace=True)\n",
    "numeric_cols = list(set(type1.columns) - set(['subject_id']))\n",
    "type1[numeric_cols] += 1\n",
    "\n",
    "type2=type1.groupby('subject_id').pct_change()\n",
    "type2['subject_id']=cis_pd_updrs_part3['subject_id']\n",
    "type2=type2.groupby('subject_id').max().reset_index().fillna(0)\n",
    "type2.columns=['subject_id']+list('PerChange_'+type2.columns[1:])\n",
    "cis_pd_updrs_part3=pd.merge(cis_pd_updrs_part3[cis_pd_updrs_part3.Visit==0].drop(['Visit'],axis=1),type2,on='subject_id')\n",
    "print(cis_pd_updrs_part3.shape)\n",
    "cis_pd_updrs_part3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining all clinical data\n",
    "clinical_data=pd.merge(cis_pd_demo,cis_pd_updrs_part1_2_4,on='subject_id')\n",
    "clinical_data=pd.merge(clinical_data,cis_pd_updrs_part3,on='subject_id')\n",
    "print(clinical_data.shape)\n",
    "clinical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting clinical data\n",
    "clinical_data.to_csv('cispd_clinical_preprocessed.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
